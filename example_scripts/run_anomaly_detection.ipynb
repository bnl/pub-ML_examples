{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show an example of training 3 different models for unsupervised anomaly detection:\n",
    "* Elliptical Envelope\n",
    "* Local Outlier Detection\n",
    "* Isolation Forest\n",
    "\n",
    "The models are optimized via cross-validation using custom metrics defined below\n",
    "\n",
    "Since we transform the original data via Principal Components Analysis (PCA), the preparatory steps and the model itself can be combined into a pipeline:\n",
    "1) Standardization of each feature across the training set\n",
    "\n",
    "2) PCA transformation\n",
    "\n",
    "3) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit, train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from sklearn.metrics import roc_auc_score as auc\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from bnl_ml_examples.anomaly.load_data import process_files\n",
    "import pickle\n",
    "import os.path\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data \n",
    "\n",
    "Select `from_csv = True` if the data have been preprocessed and saved to a csv file already. \n",
    "\n",
    "Alternatively, select `from_csv = False` if the raw hdf5 files need to be processed. In this case, specify the path to the data directory `data_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_csv = False\n",
    "folder = Path(os.path.abspath(\"\"))\n",
    "data_dir = folder.parents[0] / \"example_data\" / \"anomaly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data is processed and saved in CSX_data.csv\n"
     ]
    }
   ],
   "source": [
    "if from_csv:\n",
    "    df = pd.read_csv(\"CSX_data.csv\", index_col=0)\n",
    "else:\n",
    "    df = process_files(path=data_dir / \"training_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roi</th>\n",
       "      <th>target</th>\n",
       "      <th>std_ts_ac1</th>\n",
       "      <th>std_ts_ac2</th>\n",
       "      <th>std_ts_ac3</th>\n",
       "      <th>std_ts_ac4</th>\n",
       "      <th>std_ts_pac1</th>\n",
       "      <th>std_ts_pac2</th>\n",
       "      <th>std_ts_pac3</th>\n",
       "      <th>std_ts_diff_ac1</th>\n",
       "      <th>...</th>\n",
       "      <th>sigma_y_ts_diff_ac2</th>\n",
       "      <th>sigma_y_ts_diff_ac3</th>\n",
       "      <th>sigma_y_ts_diff_ac4</th>\n",
       "      <th>sigma_y_ts_start_end</th>\n",
       "      <th>sigma_y_ts_diff_start_end</th>\n",
       "      <th>sigma_y_ts_std</th>\n",
       "      <th>sigma_y_ts_diff_std</th>\n",
       "      <th>intensity_std_ratio</th>\n",
       "      <th>sigma_x_std_ratio</th>\n",
       "      <th>sigma_y_std_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roi2</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.599253</td>\n",
       "      <td>0.362593</td>\n",
       "      <td>0.269760</td>\n",
       "      <td>0.329957</td>\n",
       "      <td>0.599243</td>\n",
       "      <td>0.005236</td>\n",
       "      <td>0.078831</td>\n",
       "      <td>-0.204721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014480</td>\n",
       "      <td>-0.022665</td>\n",
       "      <td>0.007817</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>1.191764</td>\n",
       "      <td>0.751490</td>\n",
       "      <td>0.750202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>roi8</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.072579</td>\n",
       "      <td>-0.013803</td>\n",
       "      <td>-0.106042</td>\n",
       "      <td>-0.058326</td>\n",
       "      <td>0.072750</td>\n",
       "      <td>-0.019119</td>\n",
       "      <td>-0.104204</td>\n",
       "      <td>-0.452686</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060720</td>\n",
       "      <td>-0.060951</td>\n",
       "      <td>-0.028771</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>0.822922</td>\n",
       "      <td>0.768164</td>\n",
       "      <td>0.728900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>roi5</td>\n",
       "      <td>anomaly</td>\n",
       "      <td>0.880915</td>\n",
       "      <td>0.673688</td>\n",
       "      <td>0.453614</td>\n",
       "      <td>0.242144</td>\n",
       "      <td>0.883729</td>\n",
       "      <td>-0.470195</td>\n",
       "      <td>-0.043319</td>\n",
       "      <td>0.369855</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043010</td>\n",
       "      <td>-0.037994</td>\n",
       "      <td>-0.101340</td>\n",
       "      <td>0.004318</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.004363</td>\n",
       "      <td>0.003119</td>\n",
       "      <td>2.768945</td>\n",
       "      <td>1.200447</td>\n",
       "      <td>1.398859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>roi7</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.383967</td>\n",
       "      <td>0.434828</td>\n",
       "      <td>0.428532</td>\n",
       "      <td>0.376452</td>\n",
       "      <td>0.385161</td>\n",
       "      <td>0.339677</td>\n",
       "      <td>0.245397</td>\n",
       "      <td>-0.545863</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.114499</td>\n",
       "      <td>0.069924</td>\n",
       "      <td>-0.028232</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.001522</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.929296</td>\n",
       "      <td>0.755616</td>\n",
       "      <td>0.730380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>roi10</td>\n",
       "      <td>normal</td>\n",
       "      <td>-0.248756</td>\n",
       "      <td>0.196437</td>\n",
       "      <td>0.341823</td>\n",
       "      <td>-0.344334</td>\n",
       "      <td>-0.257903</td>\n",
       "      <td>0.150706</td>\n",
       "      <td>0.476242</td>\n",
       "      <td>-0.650925</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057387</td>\n",
       "      <td>0.642186</td>\n",
       "      <td>-0.706376</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>0.002789</td>\n",
       "      <td>0.530559</td>\n",
       "      <td>0.551655</td>\n",
       "      <td>0.574547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     roi   target  std_ts_ac1  std_ts_ac2  std_ts_ac3  std_ts_ac4  \\\n",
       "0   roi2   normal    0.599253    0.362593    0.269760    0.329957   \n",
       "1   roi8   normal    0.072579   -0.013803   -0.106042   -0.058326   \n",
       "2   roi5  anomaly    0.880915    0.673688    0.453614    0.242144   \n",
       "3   roi7   normal    0.383967    0.434828    0.428532    0.376452   \n",
       "4  roi10   normal   -0.248756    0.196437    0.341823   -0.344334   \n",
       "\n",
       "   std_ts_pac1  std_ts_pac2  std_ts_pac3  std_ts_diff_ac1  ...  \\\n",
       "0     0.599243     0.005236     0.078831        -0.204721  ...   \n",
       "1     0.072750    -0.019119    -0.104204        -0.452686  ...   \n",
       "2     0.883729    -0.470195    -0.043319         0.369855  ...   \n",
       "3     0.385161     0.339677     0.245397        -0.545863  ...   \n",
       "4    -0.257903     0.150706     0.476242        -0.650925  ...   \n",
       "\n",
       "   sigma_y_ts_diff_ac2  sigma_y_ts_diff_ac3  sigma_y_ts_diff_ac4  \\\n",
       "0             0.014480            -0.022665             0.007817   \n",
       "1            -0.060720            -0.060951            -0.028771   \n",
       "2            -0.043010            -0.037994            -0.101340   \n",
       "3            -0.114499             0.069924            -0.028232   \n",
       "4            -0.057387             0.642186            -0.706376   \n",
       "\n",
       "   sigma_y_ts_start_end  sigma_y_ts_diff_start_end  sigma_y_ts_std  \\\n",
       "0              0.000663                   0.000355        0.000548   \n",
       "1              0.000909                   0.000681        0.000627   \n",
       "2              0.004318                   0.000485        0.004363   \n",
       "3              0.000208                   0.000588        0.001522   \n",
       "4              0.001303                   0.000413        0.001603   \n",
       "\n",
       "   sigma_y_ts_diff_std  intensity_std_ratio  sigma_x_std_ratio  \\\n",
       "0             0.000730             1.191764           0.751490   \n",
       "1             0.000860             0.822922           0.768164   \n",
       "2             0.003119             2.768945           1.200447   \n",
       "3             0.002083             0.929296           0.755616   \n",
       "4             0.002789             0.530559           0.551655   \n",
       "\n",
       "   sigma_y_std_ratio  \n",
       "0           0.750202  \n",
       "1           0.728900  \n",
       "2           1.398859  \n",
       "3           0.730380  \n",
       "4           0.574547  \n",
       "\n",
       "[5 rows x 95 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal data size: 399\n",
      "Anomaly data size: 328\n"
     ]
    }
   ],
   "source": [
    "df_normal = df[df.target == \"normal\"][:].sample(frac=1, random_state=10)\n",
    "df_anomaly = df[df.target == \"anomaly\"][:].sample(frac=1, random_state=10)\n",
    "\n",
    "print(f\"Normal data size: {df_normal.shape[0]}\")\n",
    "print(f\"Anomaly data size: {df_anomaly.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data\n",
    "* train: 80% of normal data \n",
    "* validation: 10% of normal data and 50% of anomalous data\n",
    "* test: 10% of normal data and 50% of anomalous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " training data shape: (312, 95) \n",
      " validation data shape:(203, 95) \n",
      " test data shape: (212, 95)\n"
     ]
    }
   ],
   "source": [
    "# splitting points for nomal data\n",
    "normal_split1 = (df_normal.shape[0] // 10) * 8\n",
    "normal_split2 = (df_normal.shape[0] // 10) * 1 + normal_split1\n",
    "\n",
    "train = df_normal.iloc[:normal_split1, :]\n",
    "val_normal = df_normal.iloc[normal_split1:normal_split2, :]\n",
    "test_normal = df_normal.iloc[normal_split2:, :]\n",
    "\n",
    "# splitting poin for anomaly data\n",
    "anomaly_split = df_anomaly.shape[0] // 2\n",
    "\n",
    "val_anomaly = df_anomaly.iloc[:anomaly_split, :]\n",
    "test_anomaly = df_anomaly.iloc[anomaly_split:, :]\n",
    "\n",
    "val = pd.concat([val_normal, val_anomaly]).reset_index(drop=True)\n",
    "test = pd.concat([test_normal, test_anomaly]).reset_index(drop=True)\n",
    "\n",
    "print(\n",
    "    f\"\\n training data shape: {train.shape} \\n validation data shape:{val.shape} \\n test data shape: {test.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate data into features and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns=[\"target\", \"roi\"])\n",
    "target = train[\"target\"].map({\"anomaly\": -1, \"normal\": 1})\n",
    "\n",
    "X_val = val.drop(columns=[\"target\", \"roi\"])\n",
    "target_val = val[\"target\"].map({\"anomaly\": -1, \"normal\": 1})\n",
    "\n",
    "X_test = test.drop(columns=[\"target\", \"roi\"])\n",
    "target_test = test[\"target\"].map({\"anomaly\": -1, \"normal\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct (training, validation) split to use for model optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine training and validation features and targets for them to have consistent indexes\n",
    "X_comb = pd.concat([X, X_val]).reset_index(drop=True)\n",
    "target_comb = pd.concat([target, target_val]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# split the sets again, now the indexes are consistent\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_comb, target_comb, train_size=X.shape[0], random_state=None, shuffle=False\n",
    ")\n",
    "# define the split\n",
    "split_index = [-1 if x in X_train.index else 0 for x in X_comb.index]\n",
    "pds = PredefinedSplit(test_fold=split_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a custom performance metrics\n",
    "\n",
    "The metric we select here tries to simultaneously increase recalls for anomalies and normal cases and reduce false classification for both labels. The metric can be used directly in grid search optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_recalls(y, y_pred):\n",
    "    anomaly_recall = recall_score(-y, -y_pred)\n",
    "    normal_recall = recall_score(y, y_pred)\n",
    "    anomaly_precision = precision_score(-y, -y_pred)\n",
    "    normal_precision = precision_score(y, y_pred)\n",
    "\n",
    "    score = anomaly_recall * normal_recall * normal_precision * anomaly_precision\n",
    "    return score\n",
    "\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "score = make_scorer(combined_recalls, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the models we identify the set of parameters we want to optimize and the range of the values we would like to consider. The search for the optimal set of the parameters' values is done via GridSearchCV ( exhaustive search and cross-validation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elliptic Envelope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For elliptical envelope we only vary the number of principle components and the contamination level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"pca__n_components\": np.arange(1, 40),\n",
    "    \"model__contamination\": np.linspace(0, 0.5, 20),\n",
    "}\n",
    "\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"pca\", PCA()),\n",
    "        (\"model\", EllipticEnvelope(assume_centered=False, random_state=50)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, params, n_jobs=-1, cv=pds, scoring=score, refit=False)\n",
    "_ = grid.fit(X_comb, target_comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model with the optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=3)),\n",
       "                ('model',\n",
       "                 EllipticEnvelope(contamination=0.13157894736842105,\n",
       "                                  random_state=50))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contamination = grid.best_params_[\"model__contamination\"]\n",
    "n_pca = grid.best_params_[\"pca__n_components\"]\n",
    "best_EE = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"pca\", PCA(n_pca)),\n",
    "        (\n",
    "            \"model\",\n",
    "            EllipticEnvelope(\n",
    "                assume_centered=False, random_state=50, contamination=contamination\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "best_EE.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model\n",
    "We consider confusion matrix and precision and false discovery rate of anomalous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            anomaly_M   normal_M\n",
      " anomaly_T    161        3\n",
      " normal_T       6       42\n"
     ]
    }
   ],
   "source": [
    "cmat = confusion_matrix(target_test, best_EE.predict(X_test))\n",
    "print(\n",
    "    f\"            anomaly_M   normal_M\\n \\\n",
    "anomaly_T    {cmat[0][0]}        {cmat[0][1]}\\n \\\n",
    "normal_T       {cmat[1][0]}       {cmat[1][1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall = 98.2%\n",
      "false discovery rate = 3.6%\n",
      "roc_auc = 0.93\n"
     ]
    }
   ],
   "source": [
    "print(f\"recall = {recall_score(-target_test,-best_EE.predict(X_test)):2.1%}\")\n",
    "print(\n",
    "    f\"false discovery rate = {1 - precision_score(-target_test,-best_EE.predict(X_test)):2.1%}\"\n",
    ")\n",
    "print(f\"roc_auc = {auc(-target_test,-best_EE.predict(X_test)):1.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Outlier Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"pca__n_components\": np.arange(1, 20),\n",
    "    \"model__contamination\": np.linspace(0.0, 0.5, 30),\n",
    "    \"model__n_neighbors\": np.arange(1, 25, 2),\n",
    "    \"model__leaf_size\": np.arange(1, 40, 5),\n",
    "}\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"pca\", PCA()),\n",
    "        (\"model\", LocalOutlierFactor(novelty=True, algorithm=\"kd_tree\")),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, params, n_jobs=-1, cv=pds, scoring=score, refit=False)\n",
    "_ = grid.fit(X_comb, target_comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model with the optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=3)),\n",
       "                ('model',\n",
       "                 LocalOutlierFactor(algorithm='kd_tree',\n",
       "                                    contamination=0.12068965517241378,\n",
       "                                    leaf_size=1, n_neighbors=7,\n",
       "                                    novelty=True))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = grid.best_params_\n",
    "best_LOD = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"pca\", PCA(params[\"pca__n_components\"])),\n",
    "        (\n",
    "            \"model\",\n",
    "            LocalOutlierFactor(\n",
    "                novelty=True,\n",
    "                algorithm=\"kd_tree\",\n",
    "                contamination=params[\"model__contamination\"],\n",
    "                n_neighbors=params[\"model__n_neighbors\"],\n",
    "                leaf_size=params[\"model__leaf_size\"],\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "best_LOD.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            anomaly_M   normal_M\n",
      " anomaly_T    152        12\n",
      " normal_T       4       44\n"
     ]
    }
   ],
   "source": [
    "cmat = confusion_matrix(target_test, best_LOD.predict(X_test))\n",
    "print(\n",
    "    f\"            anomaly_M   normal_M\\n \\\n",
    "anomaly_T    {cmat[0][0]}        {cmat[0][1]}\\n \\\n",
    "normal_T       {cmat[1][0]}       {cmat[1][1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall = 92.7%\n",
      "false discovery rate = 2.6%\n",
      "roc_auc = 0.92\n"
     ]
    }
   ],
   "source": [
    "print(f\"recall = {recall_score(-target_test,-best_LOD.predict(X_test)):2.1%}\")\n",
    "print(\n",
    "    f\"false discovery rate = {1 - precision_score(-target_test,-best_LOD.predict(X_test)):2.1%}\"\n",
    ")\n",
    "print(f\"roc_auc = {auc(-target_test,-best_LOD.predict(X_test)):1.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolation Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the above models, the number of available model parameters (particularly, the maximum number of features for building an isolation tree) for Isolation Forest is dependent on the number of principal components. Thus, we change the number of components in a `for`-loop and run a grid search at each step of the loop and keep track of the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pca = 0\n",
    "best_score = 0\n",
    "\n",
    "for j in range(1, 25):\n",
    "\n",
    "    params = {\n",
    "        \"model__max_features\": np.arange(1, j + 1),\n",
    "        \"model__contamination\": np.linspace(0, 0.5, 20),\n",
    "    }\n",
    "\n",
    "    pipe = Pipeline(\n",
    "        [\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"pca\", PCA(j)),\n",
    "            (\n",
    "                \"model\",\n",
    "                IsolationForest(\n",
    "                    n_estimators=300,\n",
    "                    bootstrap=True,\n",
    "                    n_jobs=-1,\n",
    "                    behaviour=\"deprecated\",\n",
    "                    random_state=10,\n",
    "                    verbose=0,\n",
    "                    warm_start=False,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    grid = GridSearchCV(pipe, params, n_jobs=-1, cv=pds, scoring=score, refit=False)\n",
    "    grid.fit(X_comb, target_comb)\n",
    "    params = grid.best_params_\n",
    "\n",
    "    pipe = Pipeline(\n",
    "        [\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"pca\", PCA(j)),\n",
    "            (\n",
    "                \"model\",\n",
    "                IsolationForest(\n",
    "                    max_samples=\"auto\",\n",
    "                    n_estimators=300,\n",
    "                    bootstrap=True,\n",
    "                    n_jobs=-1,\n",
    "                    behaviour=\"deprecated\",\n",
    "                    random_state=10,\n",
    "                    verbose=0,\n",
    "                    warm_start=True,\n",
    "                    max_features=params[\"model__max_features\"],\n",
    "                    contamination=params[\"model__contamination\"],\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    ).fit(X_train)\n",
    "\n",
    "    if grid.best_score_ > best_score:\n",
    "        best_score = grid.best_score_\n",
    "        best_pca = j\n",
    "        best_IFT = pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            anomaly_M   normal_M\n",
      " anomaly_T    160        4\n",
      " normal_T       7       41\n"
     ]
    }
   ],
   "source": [
    "cmat = confusion_matrix(target_test, best_IFT.predict(X_test))\n",
    "print(\n",
    "    f\"            anomaly_M   normal_M\\n \\\n",
    "anomaly_T    {cmat[0][0]}        {cmat[0][1]}\\n \\\n",
    "normal_T       {cmat[1][0]}       {cmat[1][1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall = 97.6%\n",
      "false discovery rate = 4.2%\n",
      "roc_auc = 0.91\n"
     ]
    }
   ],
   "source": [
    "print(f\"recall = {recall_score(-target_test,-best_IFT.predict(X_test)):2.1%}\")\n",
    "print(\n",
    "    f\"false discovery rate = {1 - precision_score(-target_test,-best_IFT.predict(X_test)):2.1%}\"\n",
    ")\n",
    "print(f\"roc_auc = {auc(-target_test,-best_IFT.predict(X_test)):1.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_EE\n",
    "\n",
    "with open(\"anomaly_detection_model.pk\", \"wb\") as model_file:\n",
    "    pickle.dump(model, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing model on new data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a model among the best predictors based on your preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data is processed and saved in test.csv\n",
      "The model characterize the data as anomaly\n"
     ]
    }
   ],
   "source": [
    "test_files = data_dir / \"test_data\"  # this file was used for the model training,\n",
    "# here is considered for demonstration only\n",
    "\n",
    "new_data = (\n",
    "    process_files(path=test_files, save_file_name=\"test.csv\")\n",
    "    .drop(columns=[\"target\", \"roi\"])\n",
    "    .values\n",
    ")\n",
    "prediction = \"anomaly\" if model.predict(new_data) == -1 else \"normal\"\n",
    "print(f\"The model characterize the data as {prediction}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
